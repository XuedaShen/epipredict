---
title: "Visualizations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Visualizations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

**NOTE: This is intentionally placed in `musings` as it is still a draft.**

Good visualizations help the user understand what is going on. One may use
`plot.covidcast_signal` from the `covidcast` package to plot US maps, including
county maps. However, `covidcast` will not support maps unrelated to the US.
One may use `plot` and `ggplot` for various plots
including scatterplots. Finally, users can add animation and interactive
features on the plot.

`epipredict` supports visualizations features that are a part of COVIDcast.
However, it is not intended for
those who use third-party data visualization software such as Power BI and
Tableau. Despite that, visualizations go alongside cmu-delphi's covidcast
library.

`fable` has plotting features. However, these do not meet these needs in terms
of forecasting COVID cases.

```{r,include=FALSE,echo=FALSE}
library(dplyr)
library(epipredict)
library(epiprocess)
library(ggplot2)
library(tidyr)
library(tidymodels)
```


## Visualizing 7-day averages

COVID cases may be underreported on certain dates if fewer people get
COVID tested on weekends, or if a place doesn't do COVID updates on weekends.
This makes the daily case count jagged.

```{r}
x <- jhu_csse_daily_subset   %>%
  filter(geo_value == "ca",
         time_value >= "2020-10-01", time_value < "2021-10-01") %>%
  dplyr::arrange(geo_value, time_value) %>%
  select(geo_value,time_value,cases,cases_7d_av)

x_gather <- x %>%
  select(-geo_value) %>%
  gather("type","count",-1)

ggplot(x_gather,aes(x=time_value,y=count,colour=type)) +
  geom_line(alpha=0.8,size=1)
```

## Understanding predictions and prediction accuracy

Visualizations allow us to understand how predictions work. We will use two
different forecasts to understand why it is useful to be able to visualize.

```{r}
xx <- jhu_csse_daily_subset %>%
  filter(geo_value == "ca",
         time_value >= "2020-10-01", time_value < "2021-10-01") %>%
  dplyr::arrange(geo_value, time_value) %>%
  select(geo_value,time_value,death_rate_7d_av,case_rate_7d_av) %>%
  mutate(interaction = death_rate_7d_av * case_rate_7d_av)

r1 <- epi_recipe(xx) %>%
  step_epi_lag(case_rate_7d_av,lag = c(0,7,14)) %>%
  step_epi_ahead(case_rate_7d_av,ahead = 7) %>%
  step_epi_naomit()

wf1 <- epi_workflow(r1, linear_reg()) %>% 
  fit(xx)

p1 <- predict(wf1,new_data = xx)
```

Here's another prediction that uses death rate and the interaction with
case rate and death rate.

```{r}
r2 <- epi_recipe(xx) %>%
  step_epi_lag(case_rate_7d_av,lag = c(0,7,14)) %>%
  step_epi_lag(death_rate_7d_av,lag = c(0,7,14)) %>%
  step_epi_lag(interaction,lag = c(0,7,14)) %>%
  step_epi_ahead(case_rate_7d_av,ahead = 7) %>%
  step_epi_naomit()

wf2 <- epi_workflow(r2, linear_reg()) %>% 
  fit(xx)

p2 <- predict(wf2,new_data = xx)
```

We want to understand how predictions are different from what actual happens,
as this may enable us to know why a model is not fully accurate.

```{r}
pred_vs_actual <- xx %>%
  select(-death_rate_7d_av, -interaction) %>%
  full_join(p1) %>%
  rename(pred1 = .pred) %>%
  full_join(p2) %>%
  rename(pred2 = .pred)

pva_gather <- pred_vs_actual %>%
  select(-geo_value) %>%
  gather("type","count",-1)

ggplot(pva_gather,aes(x=time_value,y=log(count),colour=type)) +
  geom_line(alpha=0.8,size=1) +
  labs(x="Date",y="Log 7-day case rate")
```

This chart shows how the predictions differ from the actual values, but it
is rather cluttered. Here, we use a log scale and calculate the difference
between the prediction from the actual value.

```{r}
inaccuracy <- pred_vs_actual %>%
  drop_na() %>%
  mutate(p1_inaccuracy = log(pred1) - log(case_rate_7d_av),
         p2_inaccuracy = log(pred2) - log(case_rate_7d_av)) %>%
  select(-geo_value,-case_rate_7d_av,-pred1,-pred2)

i_gather <- gather(inaccuracy,"prediction","log_divided_error",-time_value)

ggplot(i_gather,aes(x=time_value,y=log_divided_error,colour=prediction)) +
  geom_line(alpha=0.8,size=1)
```

Here, we can see that p1 appears to be biased between around April to July 2021,
which may imply underfitting.
While p2 appears to be much less biased, it has jitters when predicting around
that period, which may imply overfitting.

One may want to take out non-statistically significant parameters or 
use LASSO regression to take out unnecessary parameters.

## evalcast

```{r, eval = FALSE}
options(timeout = 3600) # Times out after 1 hour rather than 1 minute

remotes::install_github("cmu-delphi/covidcast", ref = "main",
                         subdir = "R-packages/evalcast")
```

## covidHUButils

```{r, eval = FALSE}
options(timeout = 3600) # Times out after 1 hour rather than 1 minute

devtools::install_github("reichlab/zoltr",force=TRUE)
remotes::install_github("epiforecasts/scoringutils",force=TRUE)
remotes::install_github("reichlab/covidData")
remotes::install_github("reichlab/covidHubUtils",force=TRUE)
```

