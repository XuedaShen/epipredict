---
title: "Using epipredict on non-epidemic panel data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using epipredict on non-epidemic panel data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=F}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

```{r libraries}
library(dplyr)
library(parsnip)
library(recipes)
library(epiprocess)
library(epipredict)
library(ggplot2)
library(gridExtra)
```

[Panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, 
contain cross-sectional measurements of subjects over time. The `epipredict` 
package is most suitable for running forecasters on epidemiological panel data. 
A built-in example of this is the [`case_death_rate_subset`](
  https://cmu-delphi.github.io/epipredict/reference/case_death_rate_subset.html) 
dataset, which contains daily state-wise measures of `case_rate` and 
`death_rate` for COVID-19 in 2021:

```{r epi-panel-ex, include=T}
head(case_death_rate_subset, 3)
```

`epipredict` functions work with data in [`epi_df`](
  https://cmu-delphi.github.io/epiprocess/reference/epi_df.html) 
format. Despite the stated goal and name of the package, other panel datasets 
are also valid candidates for `epipredict` functionality, as long as they are 
in `epi_df` format.

```{r employ-stats, include=F}
year_start <- min(grad_employ_subset$time_value)
year_end <- max(grad_employ_subset$time_value)
```

# Example panel data overview

In this vignette, we will demonstrate using `epipredict` with employment panel
data from Statistics Canada. We will be using 
[
  Table 37-10-0115-01: Characteristics and median employment income of 
  longitudinal cohorts of postsecondary graduates two and five years after 
  graduation, by educational qualification and field of study (primary 
  groupings)
](https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=3710011501). 

The full dataset contains yearly median employment income two and five years 
after graduation, and number of graduates. The data is stratified by 
variables such as geographic region (Canadian province), education, and 
age group. The year range of the dataset is `r year_start` to `r year_end`, 
inclusive. The full dataset also contains metadata that describes the 
quality of data collected. For demonstration purposes, we make the following 
modifications to get a subset of the full dataset:

* Only keep provincial-level geographic region (the full data also has 
"Canada" as a region)
* Only keep "good" or better quality data rows, as indicated by the [`STATUS`](
  https://www.statcan.gc.ca/en/concepts/definitions/guide-symbol) column
* Choose a subset of covariates and aggregate across the remaining ones. The 
chosen covariates are age group, and educational qualification.

Below is the query for obtaining the full data and code for subsetting it as we 
just described:

```{r employ-query, eval=F}
library(cansim)

# Get statcan data using get_cansim, which returns a tibble
statcan_grad_employ <- get_cansim("37-10-0115-01")

gemploy <- statcan_grad_employ %>%
  # Drop some columns and rename the ones we keep
  select(c(
    "REF_DATE", "GEO", "VALUE", "STATUS", "Educational qualification",
    "Field of study", "Gender", "Age group", "Status of student in Canada",
    "Characteristics after graduation", "Graduate statistics"
  )) %>%
  rename(
    "geo_value" = "GEO",
    "time_value" = "REF_DATE",
    "value" = "VALUE",
    "status" = "STATUS",
    "edu_qual" = "Educational qualification",
    "fos" = "Field of study",
    "gender" = "Gender",
    "age_group" = "Age group",
    "student_status" = "Status of student in Canada",
    "grad_charac" = "Characteristics after graduation",
    "grad_stat" = "Graduate statistics"
  ) %>%
  # The original `VALUE` column contain the statistic indicated by
  # `Graduate statistics` in the original data. Below we pivot the data
  # wider so that each unique statistic can have its own column.
  mutate(
    # Recode for easier pivoting
    grad_stat = recode_factor(
      grad_stat,
      `Number of graduates` = "num_graduates",
      `Median employment income two years after graduation` = "med_income_2y",
      `Median employment income five years after graduation` = "med_income_5y"
    ),
    # They are originally strings but want ints for conversion to epi_df later
    time_value = as.integer(time_value)
  ) %>%
  pivot_wider(names_from = grad_stat, values_from = value) %>%
  filter(
    # Drop aggregates for some columns
    geo_value != "Canada" &
      age_group != "15 to 64 years" &
      edu_qual != "Total, educational qualification" &
      # Keep aggregates for keys we don't want to keep
      fos == "Total, field of study" &
      gender == "Total, gender" &
      student_status == "Canadian and international students" &
      # Since we're looking at 2y and 5y employment income, the only
      # characteristics remaining are:
      # - Graduates reporting employment income
      # - Graduates reporting wages, salaries, and commissions only
      # For simplicity, keep the first one only
      grad_charac == "Graduates reporting employment income" &
      # Only keep "good" data
      is.na(status) &
      # Drop NA value rows
      !is.na(num_graduates) & !is.na(med_income_2y) & !is.na(med_income_5y)
  ) %>%
  select(-c(status, gender, student_status, grad_charac, fos))
```

To use this data with `epipredict`, we need to convert it into `epi_df` format 
using [`as_epi_df`](
  https://cmu-delphi.github.io/epiprocess/reference/as_epi_df.html) 
with additional keys. In our case, the additional keys are `age_group`,
and `edu_qual`. Note that in the above modifications, we encoded `time_value` 
as type `integer`. This lets us set `time_type = "year"`, and ensures that
lag and ahead modifications later on are using the correct time units. See the
[`epi_df` documentation](
  https://cmu-delphi.github.io/epiprocess/reference/epi_df.html#time-types) for 
a list of all the `type_type`s available.

```{r convert-to-epidf, eval=F}
grad_employ_subset <- gemploy %>%
  tsibble::as_tsibble(
    index = time_value,
    key = c(geo_value, age_group, edu_qual)
  ) %>%
  as_epi_df(
    geo_type = "custom", time_type = "year",
    additional_metadata = c(other_keys = list("age_group", "edu_qual"))
  )
```

```{r data-dim, include=F}
employ_rowcount <- format(nrow(grad_employ_subset), big.mark = ",")
employ_colcount <- length(names(grad_employ_subset))
```

Now, we are ready to use `grad_employ_subset` with `epipredict`. 
Our `epi_df` contains `r employ_rowcount` rows and `r employ_colcount` columns.
Here is a quick summary of the columns in our `epi_df`:

* `time_value` (time value): year in YYYY format 
* `geo_value` (geo value): province in Canada
* `num_graduates` (raw, time series value): number of graduates 
* `med_income_2y` (raw, time series value): median employment income 2 years 
after graduation 
* `med_income_5y` (raw, time series value): median employment income 5 years 
after graduation  
* `age_group` (key): one of two age groups, either 15 to 34 years, or 35 to 64 
years 
* `edu_qual` (key): one of 32 unique educational qualifications, e.g., 
"Master's disploma"

```{r preview-data, include=T}
# Rename for simplicity
employ <- grad_employ_subset
sample_n(employ, 6)
```

In the following sections, we will go over preprocessing the data in the 
`epi_recipe` framework, and fitting a model and making predictions within the 
`epipredict` framework and using the package's canned forecasters.

# A Simple AR(3) Model Example

## Preprocessing 

As a simple example, let's work with the `num_graduates` column for now. We will 
first pre-process by "standardizing" each numeric column by the total within 
each group of keys. We do this since those raw numeric values will vary greatly
from province to province since there are large differences in population.

```{r employ-small, include=T}
employ_small <- employ %>%
  group_by(geo_value, age_group, edu_qual) %>%
  # Select groups where there are complete timeseries values
  filter(n() >= 6) %>%
  mutate(
    num_graduates_prop = num_graduates / sum(num_graduates),
    med_income_2y_prop = med_income_2y / sum(med_income_2y),
    med_income_5y_prop = med_income_5y / sum(med_income_5y)
  ) %>%
  ungroup()
head(employ_small)
```

Below is a visualization for a sample of the small data. Note that some groups 
do not have any time series information since we filtered out all timeseries 
with incomplete dates.

```{r employ-small-graph, include=T, eval=T}
employ_small %>%
  filter(geo_value %in% c("British Columbia", "Ontario")) %>%
  filter(grepl("degree", edu_qual, fixed = T)) %>%
  ggplot(aes(x = time_value, y = num_graduates_prop, color = geo_value)) +
  geom_line() +
  facet_grid(rows = vars(edu_qual), cols = vars(age_group)) +
  xlab("Year") +
  ylab("# of graduates as proportion of sum within group") +
  ggtitle("Trend in # of Graduates by Age Group and Education in BC and ON")
```

We will predict the "standardized" number of graduates (a proportion) in the 
next year (time $t+1$) using an autoregressive model with three lags (i.e., an 
AR(3) model). Such a model is represented algebraically like this: 

\[
  x_{t+1} = 
  \phi_0 + \phi_1 x_{t} + \phi_2 x_{t-1} + \phi_3 x_{t-2} + \epsilon_t  
\]

where $x_i$ is the proportion of graduates at time $i$, and the current time is 
$t$.

In the preprocessing step, we need to create additional columns in `employ` for 
each of $x_{t+1}$, $x_{t}$, $x_{t-1}$, and $x_{t-2}$. We do this via an 
`epi_recipe`. Note that creating an `epi_recipe` alone doesn't add these 
outcome and predictor columns; the recipe just stores the instructions for 
adding them. 

Our `epi_recipe` should add one `ahead` column representing $x_{t+1}$ and 
3 `lag` columns representing $x_{t}$, $x_{t-1}$, and $x_{t-2}$. Also note that 
since we specified our `time_type` to be `year`, our `lag` and `lead`
values are both in years. 

```{r make-recipe, include=T, eval=T}
r <- epi_recipe(employ_small) %>%
  step_epi_ahead(num_graduates_prop, ahead = 1) %>% # lag & ahead units in years
  step_epi_lag(num_graduates_prop, lag = 0:2) %>%
  step_epi_naomit()
r
```

Let's apply this recipe using `prep` and `bake` to generate and view the `lag` 
and `ahead` columns.

```{r view-preprocessed, include=T}
# Display a sample of the preprocessed data
baked_sample <- r %>%
  prep() %>%
  bake(new_data = employ_small) %>%
  sample_n(5)
baked_sample
```

We can see that the `prep` and `bake` steps created new columns according to 
our `epi_recipe`:

- `ahead_1_num_graduates_prop` corresponds to $x_{t+1}$
- `lag_0_num_graduates_prop`, `lag_1_num_graduates_prop`, and 
`lag_2_num_graduates_prop` correspond to $x_{t}$, $x_{t-1}$, and $x_{t-2}$ 
respectively.

## Model fitting and prediction

Since our goal for now is to fit a simple autoregressive model, we can use
[`parsnip::linear_reg()`](
  https://parsnip.tidymodels.org/reference/linear_reg.html) with the default 
engine `lm`, which fits a linear regression using ordinary least squares. 

We will use `epi_workflow` with the `epi_recipe` we defined in the 
preprocessing section along with the `parsnip::linear_reg()` model. Note again 
that `epi_workflow` is a container and doesn't actually do the fitting. We have 
to pass the workflow into `fit()` to get our model coefficients 
$\phi_i, i=0,...,3$.

```{r linearreg-wf, include=T}
wf_linreg <- epi_workflow(r, parsnip::linear_reg()) %>%
  parsnip::fit(employ_small)
wf_linreg
```

This output tells us the coefficients of the fitted model; for instance, 
the intercept is $\phi_0 = 0.24804$ and the coefficient for $x_{t}$ is 
$\phi_1 = 0.06648$.

Now that we have our workflow, we can generate predictions from a subset of our 
data. For this demo, we will predict the number of graduates using the last 2 
years of our dataset.

```{r linearreg-predict, include=T}
latest <- employ_small %>% filter(time_value >= max(time_value) - 2)
preds <- stats::predict(wf_linreg, latest) %>% filter(!is.na(.pred))
# Display a sample of the prediction values, excluding NAs
preds %>% head()
```

We can do this using the `augment` function too. Note that `predict` and 
`augment` both still return an `epi_df` with all of the keys that were present 
in the original dataset.

```{r linearreg-augment, include=T}
employ_small_with_preds <- augment(wf_linreg, latest)
employ_small_with_preds %>% head()
```

## Model Diagnostics 

First, we'll plot the residuals (that is, $y_{t} - \hat{y}_{t}$) against the 
fitted values ($\hat{y}_{t}$).

```{r lienarreg-resid-plot, include=T, warning=F}
employ_small_with_preds <- employ_small_with_preds %>%
  mutate(resid = num_graduates_prop - .pred)

p1 <- employ_small_with_preds %>%
  ggplot(aes(x = .pred, y = resid)) +
  geom_point(size = 1.5, alpha = .8) +
  geom_smooth(method = "loess", color = "red", linetype = "dashed", size = .7) +
  xlab("Fitted values") +
  ylab("Residuals") +
  ggtitle("Plot of Fitted Values vs. Residuals in AR(3) Model")

p2 <- employ_small_with_preds %>%
  ggplot(aes(sample = resid)) +
  stat_qq(alpha = .6) +
  stat_qq_line() +
  ggtitle("Q-Q Plot of Residuals")

grid.arrange(p1, p2, ncol = 2)
```

The left plot shows us that the residuals are mostly clustered around zero, 
but do not form an even band around the zero line, indicating that the variance 
of the residuals is not constant. The right Q-Q plot shows us that the residuals 
have heavier tails than a Normal distribution. So both the constant variance and
normality assumptions of the linear model have been violated.

# Using canned forecasters

Even though we aren't working with epidemiological data, canned forecasters 
still work as expected, out of the box. We will demonstrate this with the simple
[`flatline_forecaster`](
  https://cmu-delphi.github.io/epipredict/reference/flatline_forecaster.html) 
and the direct autoregressive (AR) forecaster 
[`arx_forecaster`](
  https://cmu-delphi.github.io/epipredict/reference/arx_forecaster.html).

With canned forecasters, we don't need to manually create a recipe and workflow;
we just need to specify the lags, aheads, and some additional arguments that 
are passed in a forecast-specific way that we'll see below.

In this first example, we'll use `flatline_forecaster` to make a simple
prediction of the 2-year median income for the next year, based on one previous 
time point. This model is representated algebraically as:
\[y_{t+1} = \phi_0 + \phi_1 y_{t}\]
where $y_i$ is the 2-year median income at time $i$.

```{r flatline, include=T, warning=F}
out_fl <- flatline_forecaster(employ_small, "med_income_2y",
  args_list = flatline_args_list(
    ahead = 1L, forecast_date = as.Date("2015-01-01")
  )
)

augment(out_fl$epi_workflow, employ_small) %>% head()
```

In this second example, we'll use `arx_forecaster` to make a prediction of the 
2-year median income based on the previous two time points' 2-year median 
income _and_ 5-year median income. This model is represented algebraically as:
\[
  y_{t+1} = 
  \phi_0 + \phi_1 y_{t} + \phi_2 y_{t-1} + \phi_3 z_{t} + \phi_4 z_{t-1} 
\]
where $y_i$ is as before, and $z_i$ is the 5-year median income at time $i$.

```{r arx-lr, include=T, warning=F}
arx_args <- arx_args_list(
  lags = c(0L, 1L), ahead = 1L, forecast_date = as.Date("2015-01-01")
)

out_arx_lr <- arx_forecaster(employ_small, "med_income_2y",
  c("med_income_2y", "med_income_5y"),
  args_list = arx_args
)

out_arx_lr$predictions %>% head()
```

Other changes to the direct AR forecaster, like changing the engine, also work 
as expected. Below we use a boosted tree model instead of a linear regression.

```{r arx-rf, include=T, warning=F}
out_arx_rf <- arx_forecaster(
  employ_small, "med_income_2y", c("med_income_2y", "med_income_5y"),
  trainer = parsnip::boost_tree(mode = "regression", trees = 20),
  args_list = arx_args
)

out_arx_rf$predictions %>% head()
```
