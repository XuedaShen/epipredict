---
title: "Model comparisons and parameter tuning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model comparisons}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>",
  out.width = "100%")
```



In this vignette, we're going to look at using k-fold cross validation to compare
which of two models is more useful in predicting COVID-19 death rates and then 
we'll do some hyperparameter tuning on the chosen model.

```{r load-packages, message=FALSE}
library(epiprocess)
library(epipredict)
library(tidyr)
library(ggplot2)
library(parsnip)
library(stringr)
#library(timetk)
library(rsample)
library(recipes)
library(dials)
library(purrr, include.only = 'pluck')
library(broom, include.only = 'tidy')
library(tune)
library(glmnet)
library(dplyr)
theme_set(theme_bw())
```


The dataset we'll be using is `case_death_rate_subset`, which contains confirmed 
COVID-19 cases and deaths from Dec 31, 2020 to Dec 31, 2021 from reports made 
available by Johns Hopkins University. To simplify things, we'll just use the 
data for California from October 1, 2021 to December 31, 2021, inclusive:

```{r}
x <- case_death_rate_subset
```

Let's suppose we want to compare whether having more lags is useful in predicting 
7 day ahead death rates. Using cross validation will help us determine
this. There are many packages that can
construct the CV splits (e.g. [`{rsample}`](https://rsample.tidymodels.org) or
[`{timetk}`](https://business-science.github.io/timetk/)), but perhaps the easiest
is to use the `epi_slide()` in `{epiprocess}`. One could easily perform proper 
validation with `epix_slide()` and an `epi_archive`, but for simplicity, 
we'll just use the `epi_df` mentioned above.


We will use rolling 1-month periods of consecutive data to train the models and
create predictions for 1-week ahead. 

```{r fcast-dates}
fcast_dates <- seq(as.Date("2021-02-27"), as.Date("2021-12-15"), by = "2 weeks")
```

We'll make predictions on every other Sunday, beginning at the end of February
resulting in `r length(fcast_dates)` forecasts.

```{r silde-him, warning=FALSE}
two_weeks <- epi_slide(
  x, ~arx_forecaster(
    .x, "death_rate", c("case_rate", "death_rate"), 
    args_list = arx_args_list(
      lags = c(0, 7), # use only 2 weeks of lags
      levels = c(.01, .025, 1:19/20, .975, .99)
    ))$predictions %>%
    select(.pred, .pred_distn, target_date),
  n = 30, # days of training data for each location
  ref_time_values = fcast_dates,
  new_col_name = "pred")

three_weeks <- epi_slide(
  x, ~arx_forecaster(
    .x, "death_rate", c("case_rate", "death_rate"),
    args_list = arx_args_list(
      levels = c(.01, .025, 1:19/20, .975, .99)
    ))$predictions %>%
    select(.pred, .pred_distn, target_date),
  n = 30,
  ref_time_values = fcast_dates,
  new_col_name = "pred"
)

all_preds <- bind_rows(
  two_weeks = two_weeks, three_weeks = three_weeks, .id = "forecaster") %>%
  rename_with(~str_extract(.x, "(?<=_)[^\\]]+"), starts_with("pred")) %>%
  select(-case_rate, -death_rate)
```

To compare the forecasters, we simply add back the observed death rate aligned
to the `target_date`. Then we can compute errors in various ways. Below,
we compute the absolute error of our point predictions and the weighted
interval score of the distributional prediction.

```{r evalcast}
all_preds <- all_preds %>%
  left_join(x %>% select(geo_value, time_value, death_rate),
            by = c("geo_value", "target_date" = "time_value")) %>%
  mutate(absolute_error = abs(.pred - death_rate),
         wis = weighted_interval_score(.pred_distn, death_rate))
```

The overall performance of the two forecasters is easily examined

```{r headline-summary}
all_preds %>% 
  group_by(forecaster) %>%
  summarise(absolute_error = mean(absolute_error), wis = mean(wis)) %>%
  knitr::kable()
```

We can also investigate how the performance depends on time, averaged over all
states and territories.

```{r temporal-plot}
all_preds %>%
  pivot_longer(absolute_error:wis, names_to = "metric") %>%
  group_by(forecaster, target_date, metric) %>%
  summarise(value = mean(value)) %>%
  ggplot(aes(target_date, value, colour = forecaster)) +
  geom_line(aes(linetype = metric)) +
  scale_colour_manual(values = c("dodgerblue", "orange")) +
  theme(legend.position = "bottom")
```

Based on this figure, there's not a major difference between the two forecasters.
The one based on three weeks of lags makes comparable large errors in April
and again in late-November. But they seem reasonable similar otherwise.
We can also compare the errors spatially, averaging over time.

```{r spatial-plot}
data("state_census")
st <- map_data("state") %>%
  left_join(mutate(state_census, region = tolower(name)), )
geo_summary <- all_preds %>%
  pivot_longer(absolute_error:wis, names_to = "metric") %>%
  group_by(forecaster, geo_value, metric) %>%
  summarise(value = mean(value), .groups = "drop") 
ggplot(left_join(st, geo_summary, by = c("abbr" = "geo_value")), 
       aes(x = long, y = lat, group = abbr)) +
  geom_polygon(aes(fill = value)) +
  facet_grid(forecaster ~ metric) +
  scale_fill_viridis_c()
```

These maps aren't super helpful to compare the forecasters, but they do reveal
that Oklahoma was forecast particularly poorly.

## Tuning hyperparameters

Selecting hyperparameters in-sample, is a slightly different procedure, and 
is easiest to do with the `{tune}` package. We'll focus on choosing the 
lasso hyperparameter in `{glmnet}`. In the next example, we will use `rolling_origin` from the `rsample` package for 
cross-validation data splitting (since this is time series) and hyperparameter tuning.


The first step is to note that, in `glmnet`, the lambda parameter is mapped
to `penalty` in the `{parsnip}` model specification. We want to indicate that
it will be tuned rather than fixed. So we use the following specification.

```{r}
lasso_pre <- linear_reg(engine = "glmnet", penalty = 1, mixture = 1) 
lasso <- linear_reg(engine = "glmnet", penalty = tune(), mixture = 1) 
```

Think of `tune()` in the above as a placeholder. After the tuning process, 
we will select a single numeric value for `lambda`. 
We will now create a random grid of tuning parameter combinations to choose
from. The size parameter controls the number of parameter combinations 
returned in the random grid.

For hyperparameter tuning, cross-validation is done "manually" rather than 
automatically using something like `cv.glmnet()`. Even so, it's verbose, but 
not terribly complicated.

```{r}
small_data <- x %>% filter(time_value < "2021-03-01") # ~ 60 days of data x 56 locs
fcaster <- arx_forecaster(
  small_data, "death_rate", c("case_rate", "death_rate"),
  trainer = lasso_pre,
  args_list = arx_args_list(lags = c(1:21), levels = 1:19 / 20)
)

wf <- fcaster$epi_workflow
m <- workflows::extract_mold(wf)
```


The `m` or `mold` object above contains the results of processing the training
data for prediction. If we want, at this point, we can abort the whole
`workflow` / `recipe` / `tidymodels` game and go right to back to our handy-dandy
`cv.glmnet()`.

```{r brute-force-cv}
out <- glmnet::cv.glmnet(as.matrix(m$predictors), m$outcomes[[1]])
plot(out)
```

This is, unfortunately, the entirely incorrect thing to do. The reason is that
the preprocessing is done to the "entire" data, when it should be done differentially
to the different training sets. (Note that this problem is exacerbated by the 
fact that we're using finalized data rather than vintage data, but we'll continue
to ignore that at our peril.)


```{r}
# nothing below works yet, as best I can tell
knitr::knit_exit()
```


```{r the-workflow}
wide_mod <- epi_recipe(small_data) %>%
  step_epi_lag(case_rate, death_rate, lag = 1:21) %>%
  step_epi_ahead(death_rate, ahead = 7) %>%
  step_epi_naomit()

the_folds <- rsample::group_vfold_cv(small_data, group = geo_value, v = 5)
penalty_grid <- data.frame(penalty = workflows::extract_fit_parsnip(wf)$fit$lambda)

tune_lasso <- tune::tune_grid(epi_workflow(wide_mod, lasso, f), 
                              the_folds, grid = penalty_grid)
```

Note that the data needs to be preprocessed before it is passed into the 
workflow. We then using rolling origin forecast resampling to produce
samples that take 60 days of data and use the next day as assessment
data. Finally, we do model tuning via grid search. The `tune_grid()` 
function computes performance metrics (ex. RMSE) for our pre-defined 
set of tuning parameters that correspond to the model (specified 
below) across the resamples.

```{r}
# Preprocess data
preprocessed <- bake(prep(r_less, x), x) %>% na.omit()

# Perform rolling origin forecast resampling
roll_rs <- rsample::rolling_origin(
  preprocessed, 
  initial = 60, 
  assess = 1,
  cumulative = FALSE
  )

# Add formula and model to workflow
wf = epi_workflow() %>%
workflows::add_formula(ahead_7_death_rate ~ lag_0_case_rate + lag_7_case_rate +
              lag_0_death_rate + lag_7_death_rate) %>%
workflows::add_model(tune_spec) 

# Model tuning by grid search
grid_res <- tune::tune_grid(
  object = wf,
  resamples = roll_rs,
  grid = grid
)

```

The metrics tied to each set of hyperparameters can be accessed via:
```{r}
grid_res$.metrics
```
Although it may be good to see all performance estimates for each 
hyperparameter combination, it can be rather cumbersome and not 
fun to comb through.

Fortunately, we may use `show_best()` to display the top 5 models along 
with estimates of their performance. Note that the models are sorted according 
to a specified metric. In our case, we'll use RMSE. We may also use 
`select_best()` to select the combination of hyperparameters with the best 
results numerically.

```{r}
show_best(grid_res, metric = "rmse")

best_hyperparam <- select_best(grid_res, metric = "rmse")
```

Now, we'll specify these parameters in a tibble and then use the
`finalize_workflow()` function to integrate these into our workflow:

```{r}
linear_param <- tibble(penalty = best_hyperparam$penalty,
                       mixture = best_hyperparam$mixture)

final_wf <- 
  wf %>% 
  finalize_workflow(linear_param)
final_wf
```

We are now all set to fit the model to a training set and 
use that to make predictions.
